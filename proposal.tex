\section{Proposal} \label{sec:proposal}

Let us assume the resources available are fairly fixed in line with \secref{sec:resources}.
Hence we will need a resource allocation  committee \secref{sec:arbitration} to arbitrate requests.

We promised some form of user batch which it seems likely will be needed at least initially as many users understand batch processing of images say. By batch here we should be clear we mean a BPS command line interface to something like PanDA. \secref{sec:bpsbatch}

These two items with the RSP fulfill our requirements.  RSP covers a bunch of other requirements on queries and image access \secref{sec:rsp}.


\subsection{Science Platform}\label{sec:rsp}
The RSP is now well established and is following the vision \citeds{LSE-319}.
Data Preview 0 has introduced an early version of this to the community.

\citeds{DMTN-202} point 4 (\secref{sec:requirements}) in respect of catalogs we now propose to be satisfied by some form of Dask/Spark access to the catalogs as spatially sharded Parquet files.
The previous baseline design for this was a layer on top of Qserv, but in practice we do not have this ``next to the database processing'' implemented, and the RSP and Data Access teams feel that, with the advance of data-science software, a Parquet-based solution is now a good solution.
There is some effort available in FY24 to prototype and implement this.
We shall consider carefully what the minimum system we need here is to cover our requirements.

We will not at this point promise extensive Dask/Spark like services.
The minimum needed to meet requirements will be done on construction and we shall  work further  on it in the background with LINCC.
We all agree this will be scientifically useful, but we need to finish construction as a priority and this is not a requirement. We will work with LINCC on it certainly and something will be available but we are not promising this and we are not accepting requirements on it.

The standard/default allocation for any data rights user will be RSP access.
Users with bulk needs will use BPS (\secref{sec:bpsbatch}) at a Data Facility.


\subsection{Butler Gen3} \label{sec:gen3}

Whether Batch or RSP a butler is available with provenance information which should cover DMS-REQ-0106 and DMS-REQ-0121.
If users do not use the Butler they will not have provenance.

\subsection{BPS Batch }\label{sec:bpsbatch}

Our baseline is user batch based on Gen3 Task framework.
Users needing batch will need to go via the Resource Allocation Committee (\secref{sec:arbitration}) and
will be given SLAC accounts to run user Batch jobs at SLAC.
Those who do not wish to use the Rubin pipelines could be facilitated see \secref{sec:othercode}.


We set DP0.2 as an initial test of seeing how we could work with PanDA \citeds{RTN-013}
for DRP. Potentially it could also be used for Prompt Processing and user batch processing.
In fact we use the BPS front end to submit jobs and middleware which to keep BPS working with at least on other back end as well as PanDA.

It seems then the minimum promise to our batch users would be a BPS front end for job submission.
This would make it look the same as any of our internal processes.
In the end if we use Condor, Slurm, the submission would be BPS.
The job tracking of course would depend on the back end.
We should not necessarily commit to the same back end in all locations - hence USDF may be PanDA but on Google or an processing IDAC we may have a different back end.

This BPS fronted system would meet \citeds{DMTN-202} points 1-5 (\secref{sec:requirements}). The Point 4 on catalogs can be met with Qserv.
This would also cover DMS-REQ-0119, DMS-REQ-0125, DMS-REQ-0128, DMS-REQ-0123, DMS-REQ-0127.

\subsubsection{Interactive vs Batch} \label{sec:interactivevbatch}
User will need to consider when they should switch to batch.
This will largely be a function how long the user is willing to wait,` how big their job is and how repeatable it is.
Processing say a full focal plane image on RSP with interactive processing via DASK should take some minutes and should be fine.
Should one wish to process ten focal planes (hence > 1K images)  it may become tedious and be more suited to batch. i
That of course means  one is not tweaking the code each time.
We will have to settle on a guide number for where switching to batch makes more sense but between 500 and 1K images would
seem to be the point where a user is falling more in the batch realm.
Also if one is processing 1K images it will probably soon be 2 or 3K or more.



\subsubsection{Non Rubin code} \label{sec:othercode}
Users may wish to run arbitrary code on Rubin images.
It should be possible to provide a butler recipe to get an image or set of images to the users space and allow them to run any code which takes FITS images.
Done in a container this could be done for a large volume of images - it would have to get past  the resource allocation committee of course.
This would require custom work by the user and would not be a general facility.
Run in this mode little provenance would be kept.  (See \secref{sec:gen3})

Some provenance could be garnered by ingesting the generated files which could at least give the execution graph of their creation.
If users wish only to write files they would have to go in their user space or potentially a VOSpace end point - the user would need to organize and track such files themselves.


\subsection{Resource Allocation Committee}\label{sec:arbitration}
As \secref{sec:resources} points out we will need an arbitration committee of some sort.
It seems this should also favor individuals from under represented  and under resourced groups though that is being proposed here for the first time.
We note also  proposals committing to releasing Level 3 data products that are made available to all data rights holders will also  be allowed to be given a preference by the RAC.
The committee is called out in the operations plan \citedsp{RDO-018} Section 12.2 but we have as yet not created a charge for it.
This committee would at least be responsible for creating, implementing and updating  policies on at least:

\begin{itemize}
\item Requests for storage over the default allocation, either temporary or permanent.
\item Requests for large batch compute accounts at SLAC.
\item Requests for extra batch compute cycles on Google.
\item Bringing compute from another Google funded account to operate on the Rubin Object Store Cache or Science Platform.
\item Cache Optimization on Google wrt. which data sets may live entirely on Google (e.g. coadds).
\item \ldots
\end{itemize}

This would meet \citeds{DMTN-202} point 6  (\secref{sec:requirements}).

\subsection{Other catalogs}

DMS-REQ-0124: Federation with external catalogs, is a bit open ended.
Theoretically we already meet this with IVOA protocols since we could match to any IVOA service.
DAX will provide a small user catalog match to Qserv.
Other catalogs could be loaded and matched however getting a list has proved fairly inconclusive.
LINCC may again help here by coordinating IDACs to provide neighbor tables/services for other catalogs.
One notion would be to have a the Object catalog at IDACs matched to their local holdings.
We could consider migrating some of those neighbor catalogs back to USDF Qserv.
Gaia will be provided as part of the processing - there is no requirement listing catalogs so from a verification perspective we can put this on IDACs

